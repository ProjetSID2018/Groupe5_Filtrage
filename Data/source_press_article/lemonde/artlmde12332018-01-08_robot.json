{"title": "Facebook lève un peu plus le voile sur ses pratiques de modération", "newspaper": "Le Monde", "author": "Damien Leloup", "date_publi": "30/06/2017", "content": "Deux milliards d’utilisateurs, et des dizaines de milliers de messages haineux :  Facebook , qui a annoncé cette semaine qu’il avait atteint un nouveau record de  nombre d’utilisateurs , a également dévoilé pour la première fois quelques chiffres sur sa modération, un domaine sur lequel le groupe était jusqu’à présent resté très discret. Dans un long message publié mardi 27 juin, le grand groupe du Web explique  avoir  modéré en moyenne 66 000 messages chaque semaine signalés comme  « hate speech »  (discours de haine), une catégorie qui regroupe les incitations à la violence et à la haine, ou encore les insultes sexistes, racistes ou homophobes. Soit 288 000 messages en moyenne par mois. Mais le réseau  social  a également donné quelques clés de lecture de ces chiffres, en détaillant sa  politique  de modération sur ces sujets – et les questions qui ne sont toujours pas complètement tranchées à ce sujet.  « La première difficulté pour  arrêter  les discours de haine est de  définir  des limites ,  écrit Richard Allan , l’un des responsables de la  politique  publique de l’entreprise.  Des personnes peuvent  être  en désaccord sur des sujets comme la politique étrangère d’un Etat, ou la moralité des enseignements de certaines  religions , et nous voulons qu’ils puissent  débattre  de ces sujets sur Facebook. Mais où se situe la ligne qui sépare le débat du discours de haine ? » \n        Lire aussi :\n         \n     \n                Facebook : « Nous voulons faire d’Internet une “no-go zone” pour les terroristes »\n     \n Messages et contexte En pratique, Facebook explique  utiliser  un principe général, et des règles spécifiques dans de nombreux cas particuliers. De manière globale, le réseau social considère comme haineux tout discours qui s’attaque à des personnes en fonction de  « caractéristiques protégées » , dont le sexe, l’origine ethnique, la nationalité, la religion, l’orientation sexuelle… Mais ce principe se heurte fréquemment à des situations locales particulières. En  Italie ,  « le mot “frocio” (“pédé”) » est par exemple considéré comme du discours de haine lorsqu’il est adressé à une personne, mais il est aussi utilisé par les militants des droits LGBT pour  dénoncer  l’homophobie » , explique Facebook, qui procède à des suppressions au cas par cas en fonction du  contexte . Le contexte est, affirme Facebook, le principal élément qui doit  guider  les règles de modération. En  Allemagne , où la multiplication de messages racistes ou haineux contre les migrants avait inquiété le gouvernement après l’accueil par le pays de nombreux migrants syriens, le réseau social affirme avoir fait  évoluer  ses règles pour  « supprimer à la fois les appels à la violence contre les migrants ou les messages déshumanisant, comme ceux qui les comparaient à des animaux, à de la saleté ou à des ordures » , tout en laissant  « la possibilité pour les gens d’exprimer leur opinion sur l’immigration elle-même » . De même, le réseau social explique  faire  des exceptions pour des mots ou des expressions qui sont a priori contraires à ses règles, mais qui peuvent aussi être utilisées pour  « de l’autodérision, ou des citations de paroles de chansons » . L’ intelligence artificielle  n’est pas la panacée Une grande partie de ces règles avaient déjà été dévoilées par plusieurs journaux européens ces dernières années. La  Süddeutsche Zeitung  et, plus récemment, le  Guardian  avaient publié plusieurs documents utilisés pour la  formation  des modérateurs de Facebook, soit 4 500 personnes dans  le monde , auxquelles s’ajouteront dans l’année  avenir  3 000 salariés supplémentaires, a annoncé Facebook. Ce 28 juin, le site ProPublica avait également publié  plusieurs extraits de documents. \n        Lire aussi :\n         \n     \n                Violence, menaces, suicide… des documents internes précisent la politique de modération de Facebook\n     \n « Il est clair que la manière dont nous appliquons nos règles n’est pas parfaite , reconnaît Facebook.  Nous sommes souvent confrontés à des cas difficiles à  trancher  – et nous nous trompons trop souvent. »  Surtout parce que ces questions sont complexes, argumente Facebook, et que la surmodération comme la sous-modération posent, légitimement, des problèmes aux utilisateurs. \n        Lire aussi :\n         \n     \n                Censure a priori et liberté d’expression\n     \n En matière de modération, il n’existe pas de baguette magique ni de solution parfaite, dit le réseau social. Même l’intelligence artificielle, souvent mise en avant par le groupe comme par des gouvernements comme l’outil ultime pour  gérer  les millions de messages publiés chaque jour sur les  réseaux sociaux , est loin d’être une solution, au moins pour l’instant, reconnaît Facebook.  « La technologie continuera d’être un élément important dans nos efforts pour nous  améliorer . Mais si nous continuons d’investir dans ces avancées prometteuses, nous sommes encore loin de  pouvoir  nous  reposer  sur l’intelligence artificielle pour gérer des sujets aussi complexes et mouvants que la lutte contre les discours de haine » , écrit Richard Allan."}