{"title": "YouTube détaille ses nouvelles mesures contre le « contenu violent »", "newspaper": "Le Monde", "author": "La rédaction", "date_publi": "18/10/2017", "content": "La plate-forme de vidéo YouTube  a détaillé, mercredi 18 octobre , la mise en place de plusieurs mesures de lutte contre les  « contenus violents et extrémistes » , testées depuis cet été. En juin, le grand groupe spécialiste de la vidéo en ligne avait annoncé un programme en quatre points pour  limiter  l’impact des  vidéos  appelant à la haine et au terrorisme sur son service, après  avoir  été vivement critiqué par plusieurs pays européens au printemps sur le sujet. Les gouvernements français et britanniques avaient notamment demandé à YouTube de  supprimer  les  «  vidéos  terroristes »  beaucoup plus rapidement, y compris avant qu’elles aient pu  être  vues. \n        Lire aussi :\n         \n     \n                Lutte contre le terrorisme sur Internet : le flou des propositions d’Emmanuel Macron et Theresa May\n     \n Or, pour YouTube, comme pour la quasi-totalité des services en ligne, la modération s’effectuait jusqu’à présent quasi exclusivement sur la base des signalements d’utilisateurs. Les millions de contenus publiés chaque jour en ligne ne sont pas examinés par les modérateurs de  Facebook  ou de YouTube, seuls le sont ceux qui ont été signalés par des internautes. Pour  accélérer  la procédure, YouTube, comme d’autres  réseaux sociaux , avait expliqué en juin  compter  sur les progrès de l’ intelligence artificielle , et avait expliqué  travailler  à un logiciel de détection automatisée qui permettrait d’identifier des contenus violents ou problématiques sans  devoir  se  reposer  sur des signalements. Dans un communiqué publié cette nuit, YouTube explique avoir utilisé une base de données d’un million de signalements examinés par ses modérateurs pour « entraîner » son  logiciel , et affirme avoir obtenu d’excellents résultats.  « Plus de 83 % des  vidéos  que nous avons supprimées parce qu’elles encourageaient l’extrémisme violent en septembre ont été modérées avant qu’elles soient signalées par un humain, un chiffre en hausse de huit points par rapport au mois d’août » , dit YouTube. En revanche, YouTube ne donne pas de chiffres sur le nombre de « faux positifs », les vidéos identifiées par son logiciel comme contraires à ses règles mais qui ne le sont pas. C’est l’un des enjeux-clés de l’automatisation de la modération : les critiques de ces outils soulignent que les logiciels d’apprentissage automatique, même les plus perfectionnés, sont incapables de  comprendre  des notions comme l’humour, et risquent donc d’identifier à tort un grand nombre de contenus comme  « interdits » . La plupart des grandes  société s qui ont mis en place des outils de détection automatisée, dont YouTube, tentent de limiter ce problème en laissant le dernier mot à des modérateurs humains, le logiciel se contentant de leur  transmettre  les contenus repérés comme problématiques. Vidéos « cachées » Surtout, YouTube a détaillé un nouveau niveau de sanctions pouvant  toucher  une vidéo, déjà évoqué en juin. Jusqu’à présent, les vidéos signalées pouvaient être soit supprimées soit laissées en ligne. Désormais, les vidéos  « qui ne sont pas illégales et ne violent pas les règles de YouTube mais qui contiennent des propos religieux controversés ou suprémacistes »  peuvent se  voir  placées dans une troisième catégorie. Elles sont alors largement cachées sur la plate-forme : elles sont précédées d’un avertissement, ne peuvent plus être commentées,  « aimées » , ou  apparaître  dans les suggestions de vidéos. Ce nouveau statut doit  « nous  aider  à  trouver  un équilibre entre la  défense  de la liberté d’expression, en préservant une archive de contenus dans l’intérêt public, tout en empêchant ces vidéos d’être largement diffusées ou recommandées » , dit YouTube. Le système de recommandations de YouTube est régulièrement accusé d’encourager la création de  « bulles de filtres » , dans lesquelles le fait de  regarder  une vidéo défendant une théorie ou une thèse  politique  entraîne la suggestion de nombreuses vidéos similaires. Après la fusillade de Las Vegas au début d’octobre, la plate-forme de vidéos avait  notamment été critiquée  pour avoir mis en avant, de manière automatisée, des vidéos conspirationnistes qui pour certaines remettaient en doute jusqu’à l’évidence de l’attaque qui a fait 58 morts.", "theme": "pixels"}