{"title": "“Facebook Files” : ce que nous révèlent les manuels de modération de Facebook", "newspaper": "Telerama", "author": "Pablo Maillé", "date_publi": "22/05/2017", "content": "\n                            \n                             Ignorer ou supprimer ? C’est  l’éternel dilemme  qui se pose chaque jour aux modérateurs de Facebook. Face à des photos de drapeaux nazis, des portraits d’Oussama ben Laden ou des images d’armes à feu, que faire ? Les censurer, pour protéger les internautes les plus fragiles ou, au contraire, les laisser exister, au nom de la liberté d’expression ? Si le débat revient régulièrement sur le devant de la scène, la position officielle du réseau social paraissait jusqu’alors assez floue, virant parfois à l’absurde. En 2011, par exemple, Facebook avait censuré  L’Origine du monde , le célèbre tableau de nu féminin de Gustave Courbet… mais se disait  réticent , quelques années plus tard, à supprimer des vidéos djihadistes. \r\n\r\n Les choses pourraient toutefois s’éclaircir : ce dimanche 21 mai, le quotidien britannique  The Guardian  a publié des dizaines de  documents , internes à Facebook, qui détaillent la politique de modération du réseau. Le média s’est procuré pour la première fois des extraits des  « manuels de modération »  (jusqu’alors réservés à ses modérateurs) et en a tiré une série de dix articles plus détaillés. Voici trois enseignements à tirer de ces documents. \r\n\r\n Les appels au meurtre contre Donald Trump censurés \r\n\r\n Vous n’appréciez pas (mais alors, vraiment pas) Donald Trump ? Prenez garde : des publications comme  « Quelqu’un devrait tirer sur Trump »  peuvent être  supprimés  par Facebook. La raison ? Donald Trump est un chef d’Etat ; une menace le concernant représente donc, d’après les règles du réseau social, un  « risque crédible » , au même titre que s’il s’agit de n’importe quel chef d’Etat ,  d’un activiste ou d’un journaliste (entre autres). De même, si la menace concerne des sans-abris ou des étrangers en tant que groupe social, elle doit être censurée par les modérateurs du réseau. En revanche, des menaces du type  « J’espère que quelqu’un te tuera » ,  « Quelqu’un devrait battre un roux »  ou  « Allons frapper des gros »  sont proférées à l’intention d’utilisateurs « lambda », ne faisant pas partie de la liste des  « groupes vulnérables »  établie par Facebook. Elles sont donc tolérées, n’étant pas considérées comme des  « menaces crédibles »  mais seulement comme des expressions violentes. \r\n\r\n Décryptage MacronLeaks : quand Facebook et Twitter tentent d'endiguer les fake news \r\n\r\n Les photos de maltraitance infantile tolérées \r\n\r\n D’après les documents, la violence d’une vidéo mise en ligne ne peut justifier à elle seule sa censure. En clair, selon la  politique  du groupe de Mark Zuckerberg, certaines vidéos violentes peuvent aider à des prises de conscience. Ainsi, dans le cas des vidéos concernant la maltraitance des enfants,   elles doivent être indisponibles pour les mineurs (déclarés), et censurées seulement dans le cas où elles sont partagées avec  « sadisme et célébration » . Celles qui montrent un enfant frappé, brûlé ou étranglé par un adulte sont précédées d’une mention spécifique. Les photos de maltraitance infantile, en revanche, sont systématiquement ignorées. Selon Facebook, le but est de pouvoir venir en aide à l’enfant concerné, en maintenant en ligne des images qui permettent de l’identifier. \n                \t\n                 \r\n\r\n Le cas des tentatives de suicide en direct \r\n\r\n Au sujet des tentatives de suicide sur Facebook Live (l’outil de diffusion en direct du réseau social), récurrentes ces derniers mois, les  consignes  de l’entreprise précisent ne pas vouloir  « censurer ou punir davantage les personnes désespérées qui tentent de se suicider » .  « Les experts ont établi qu’il est préférable de laisser l’utilisateur diffuser en direct tant qu’il est en contact avec ses spectateurs »  précise le document. Cependant, les modérateurs ont désormais pour consigne de  « supprimer toutes les vidéos dépeignant le suicide – sauf si elles ont une valeur informative – y compris quand ces vidéos sont partagées par quelqu’un d’autre que la victime pour attirer l’attention »  (la distinction entre un suicide « ayant » une valeur informative et un suicide « dépourvu » de valeur informative n’étant pas précisée…). Exemple de cas particulier, faisant exception : en octobre 2016, le réseau social a choisi de laisser en ligne une vidéo montrant un citoyen égyptien en train de s’immoler par le feu pour protester contre la hausse incontrôlée des prix dans son pays. \r\n\r\n Récemment, Mark Zuckerberg a annoncé que 3 000 modérateurs supplémentaires seraient  recrutés  d’ici l’an prochain, en plus des 4 500 dont il dispose aujourd’hui. Pour l’heure, d’après  The Guardian , ceux-ci disposent en moyenne de seulement de 10 secondes pour décider si un contenu est acceptable ou non… \r\n\r\n Enquête Le “digital labor”, les nouveaux temps modernes   Abo \r\n\r\n   \r\n\n\n                                                    ", "theme": "medias"}