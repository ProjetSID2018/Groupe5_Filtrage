{"title": "Contenus choquants : YouTube hypocrite ou impuissant ?", "newspaper": "Le Point.fr", "author": [], "date_publi": "05/01/2018", "content": "\nLa plateforme a adressé un avertissement à Logan Paul pour sa vidéo montrant un homme pendu. Reste que c'est son algorithme qui promeut ce sensationnalisme. Logan Paul, un youtubeur américain aux millions d'abonnés, a fait polémique en postant une vidéo montrant un cadavre pendu dans « la forêt des suicides » au Japon. Cette forêt est connue pour être un lieu où se rendent des Japonais voulant mettre fin à leurs jours. De nombreux internautes ont immédiatement reproché cette publication au vidéaste, poussant ce dernier à l'effacer et à présenter des excuses publiques. YouTube, filiale de Google, a fait de même et a indiqué que son règlement interdisait clairement la publication de ce type de contenu. Voire ! Car la chaîne de Logan Paul répond à tous les critères qui permettent d'engranger des millions de vues sur Youtube, lesquels sont assez simples : proposer des contenus sensationnels à grand renfort de titres provocateurs, parfois mensongers. Le but est de faire appel à la curiosité instinctive de l'internaute. Remplir ces critères permet de figurer dans les vidéos recommandées par YouTube sur sa page d'accueil et de maximiser la visibilité. Ainsi voici la page d'accueil de YouTube ce vendredi matin en fin de matinée. \n Si ces caractéristiques permettent d'arriver dans les recommandations Youtube, c'est parce que le site fonctionne avec un algorithme dont il est l'unique propriétaire. Les contenus mis en avant dans les recommandations sont ceux qui optimisent le plus de temps de visionnage d'un utilisateur sur la plateforme. Et donc permettent de diffuser le plus de contenus publicitaires. Bien que la recette de l'algorithme ne soit pas accessible au public, le sociologue turc Zeynep Tufekci a, dans une conférence, donné une idée précise du mode de fonctionnement : « Plus vous laissez penser que les vidéos recommandées ont un caractère sensationnel, plus les visiteurs ont une chance de rester sur le site. Et ils vont enchaîner les visionnages de vidéos. » Selon Guillaume Chaslot, un ancien employé de Google ayant travaillé sur l'intelligence artificielle de la plateforme de streaming, c'est bien l'algorithme qui encourage la prolifération accrue des « fake news » et des théories du complot dans les recommandations vidéos de Youtube. Comme il l'explique sur le blog Medium, « des milliers de vidéos sur YouTube affirment que la terre est plate. Les utilisateurs commencent à les regarder par curiosité. Certains d'entre eux ont des doutes et passent donc du temps sur YouTube pour avoir plus d'informations. Ces vidéos sont efficaces pour retenir l'attention. L'intelligence artificielle va donc recommander ces vidéos plus souvent. » YouTube a plusieurs fois affirmé agir pour que les vidéos à caractère violent, choquant, mensonger ou encore conspirationniste soient mieux repérées par l'algorithme. L'entreprise a intérêt à améliorer le contrôle de ses contenus par son algorithme, pour ne pas perdre ses annonceurs, soucieux d'éviter les « bad buzz », et éviter l'intervention des États. En Allemagne par exemple, une loi sanctionne toute plateforme numérique qui ne supprimerait pas des contenus jugés illégaux dans les 24 heures suivant leur publication. Lors de ses vœux à la presse, Emmanuel Macron a annoncé la création d'une loi allant dans le même sens pour agir contre les « fake news ». \nLire aussi Macron s'en va-t-en guerre contre les « fake news »\n\n\nL'algorithme de YouTube est-il fiable ? Chez Google, une source – qui préfère rester anonyme – estime qu'il ne manque que 20 % de contenus qui contreviennent à son règlement. Pas de chance, parmi eux se trouvait la vidéo de Logan Paul qu'il a lui-même supprimée quelque 48 heures après sa publication. Le vidéaste a cependant reçu un « strike » temporaire, un avertissement. S'il en reçoit plus de trois, YouTube fermera sa chaîne. ", "theme": "monde"}