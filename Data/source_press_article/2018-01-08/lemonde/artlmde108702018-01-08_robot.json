{"title": "Pour venir à bout des trolls, éliminez leur espace de discussion", "newspaper": "Le Monde", "author": "Luc Vinogradoff", "date_publi": "26/09/2017", "content": "Reddit, le site  qui se décrit comme  « la page d’accueil de tout Internet » , est connu dans l’écosystème des  réseaux sociaux  pour son relatif laisser-faire, sa  défense  de la liberté de parole, qui peut se  transformer  en une tolérance pour toutes sortes de comportements haineux. En juin 2015,  Reddit  annonce la suppression de deux sous-forums (appelés  « subreddits » ) parmi les dizaines de milliers qu’il héberge : r/CoonTown, lieu de blagues et menaces racistes contre les Noirs ; r/fatpeoplehate, où l’on insulte et se moque des gros. Ces deux communautés, qui rassemblaient alors 170 000 utilisateurs, n’ont pas respecté la  politique  anti-harcèlement et «  dégradent le site pour tout  le monde  » ,  justifie le PDG et cofondateur du site, Steve Huffman . Il précise quand même : «  Nous ne les avons pas bannis parce qu’ils étaient racistes .  Nous les avons bannis parce que nous passions trop de temps à  essayer  de les  gérer . » Les « comportements haineux » disparaissent C’était la première fois que Reddit appliquait  une politique de modération, dite  « de quarantaine » . Les modérateurs ne visent plus les individus qui contreviennent aux règles, mais le lieu où les  discussions  ont lieu. On ferme la cour de récré virtuelle plutôt que de  bannir  un à un les utilisateurs qui s’y rendent. Deux ans plus tard, des chercheurs américains ont tenté de savoir si cette technique  a été efficace : a-t-elle permis de  « réduire les comportements haineux »  ou n’a-t-elle que facilité  « le transfert de tels comportements à d’autres parties du site »  ? Les chercheurs ont examiné 100 millions de commentaires sur les deux sous-forums bannis et en ont extrait un  « lexique de termes haineux » , qui y étaient régulièrement utilisés. Un algorithme, combiné avec un affinage manuel, pour  être  sûr que les termes n’étaient pas sortis de leur  contexte , a ensuite balayé l’ensemble du site Reddit pour  savoir  si les mots du lexique étaient apparus dans d’autres sous-forums depuis le bannissement. Le premier enseignement est  « qu’une partie non négligeable des utilisateurs réguliers de ces sous-forums bannis sont devenus inactifs ».  Le second est que chez ceux qui sont restés, et dont l’activité a été étudiée, la fréquence d’utilisation de  « discours haineux »  qu’ils répandaient dans leurs anciens sous-forums  « a diminué d’au moins 80 % » . Les chercheurs écrivent  « qu’il y a de fortes chances que cette baisse soit la conséquence du bannissement, et pas un hasard ». Autrement dit, les « redditors » qui insultaient les Noirs et les gros ont soit quitté Reddit, soit intégrés d’autres communautés où ils n’ont pas importé leur comportement. Sur le fond, ces utilisateurs n’avaient sûrement pas changé. Ça les amuserait toujours d’écrire des choses horribles sur les Noirs et les gros. Mais leur comportement raciste et hostile n’apparaissait plus en public, n’étant plus toléré et encore moins encouragé hors de leurs sous-forums de prédilection.  Comme le résume Eric Gilbert, l’un des chercheurs de l’équipe  : « Ils n’ont pas banni des individus. Ils n’ont pas banni tel ou tel mot. Ils ont banni des espaces où ces mots étaient susceptibles d’être écrits. » Cela leur fait  dire  que la méthode utilisée à l’époque ( et répétée depuis ) par Reddit fonctionne, dans les bonnes conditions. Quand elle est appliquée  « à des groupes précis et petits » , elle parvient à «  réduire et  contenir  les comportements haineux » . D’autres plateformes comme  Facebook ,  Twitter  ou YouTube, qui ont tous leurs problèmes spécifiques de trolls, harcèlement et discours violents, pourraient s’en  inspirer  et être plus proactives, elles qui ont  « tendance à bannir de façon défensive, au cas par cas, souvent en réponse à des signalements d’autres utilisateurs » . Les mêmes limites que la vidéosurveillance Bannir les  lieux  de discussions  « est un outil parmi d’autres pour les plateformes sociales » , mais tout le  monde , y compris les chercheurs, est conscient que ce n’est pas la solution miracle. Une des conséquences de la méthode de quarantaine a été  « une baisse des activités des utilisateurs sur Reddit » . En termes financiers, ça veut dire simplement dire moins de trafic, donc moins de revenus pour ces compagnies. Si  lutter  au lance-flammes contre les trolls n’est pas financièrement intéressant, elles auront tendance à ne pas en  faire  une priorité. Il ne faudrait pas non plus  croire  qu’on fait  disparaître  les comportements haineux par magie, uniquement en supprimant des lieux de discussion. C’est comme croire qu’on fait disparaître les actes criminels en mettant des caméras de surveillance partout.  On ne fait que repousser le problème hors des champs surveillés . Dans le cas de Reddit, beaucoup d’utilisateurs de r/CoonTown et r/fatpeoplehate  ont migré vers d’autres sites  qui, pour  récupérer  le trafic, vantaient une liberté totale d’expression : Voat, Snapzu, Discord, par exemple. En nettoyant (une partie) de ses « écuries »,  « Reddit en a fait le problème de quelqu’un d’autre » , reconnaissent les chercheurs, et n’a  « très probablement pas eu comme conséquence qu’Internet soit plus sûr ou moins haineux ». ", "theme": "big browser"}