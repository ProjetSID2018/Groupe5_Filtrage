{"title": "Voiture autonome : par quoi serez-vous choques ?", "newspaper": "Liberation", "author": ["Jean-Francois Bonnefon"], "date_publi": "2017-10-19", "content": "Quels risques etes-vous pret a accepter a bord d'un vehicule  autonome  ? Pour vous-meme, les autres passagers et les passants ? Preferez-vous qu'une voiture renverse cinq pietons d'age mur plutot qu'un bebe ? Un homme qu'une femme ? Votre voiture devrait-elle sacrifier votre vie pour sauver une femme enceinte, meme si elle traverse au rouge ? Nous travaillons avec des chercheurs du MIT, de Harvard, et de l'universite de Californie (1) sur le simulateur Moral Machine (moralmachine.mit.edu) qui permet de collecter les preferences morales des gens dans ces situations ou une voiture autonome doit choisir entre plusieurs accidents inevitables. Disponible en dix langues, il rassemble pres de 4 millions d'utilisateurs et a deja permis de recolter 37 millions de reponses dans une centaine de pays (2). Les dilemmes ethiques de nos simulations ne sont pas entierement nouveaux : ils se rapportent au fameux dilemme du tramway, experience de pensee decrite des 1967. Elle consiste a imaginer ce que devrait faire une personne qui a la possibilite de detourner de sa route un tramway, pour l'empecher de tuer un groupe de cinq individus, mais ne peut que le rediriger vers quelqu'un d'autre qui mourra alors a leur place. Beaucoup de philosophes considerent qu'il est impossible de faire un choix moralement irreprochable dans cette situation. Cette volonte de ne pas choisir est legitime tant que l'on reste dans une experience de pensee. Mais le dilemme du tramway va se poser quand les voitures autonomes seront deployees sur les routes, et nous ne pourrons plus alors nous permettre de ne pas choisir. Le cas du vehicule autonome est unique en intelligence artificielle en ce qui concerne les decisions a caractere moral. D'abord, parce qu'avec les voitures autonomes nous confions a une machine une decision que nous n'avons nous-memes jamais prise de facon reflechie : en cas d'accident, nos reactions relevaient jusqu'ici de reflexes et d'impulsions imprevisibles. Ensuite, parce que les autres domaines (comme le medical ou le judiciaire) permettent un temps de reflexion avant d'accepter ou non le verdict de la machine, ce qu'une situation d'accident ne permet pas. Le projet Moral Machine ne pretend pas determiner ce qui est ethique ou moral. Mais il nous semble qu'avant de legiferer et mettre ces voitures sur les routes, il convient pour les pouvoirs publics et les constructeurs de savoir quelles sont les solutions les plus acceptables socialement aux yeux de la population. Ainsi, nous savons deja grace a nos premiers resultats qu'en cas d'accident inevitable, un consensus moral se forme pour minimiser le nombre de morts ou de blesses, qu'ils soient pietons ou passagers. Mais lorsque l'on demande aux utilisateurs quelle voiture ils acheteraient, ils preferent une voiture qui privilegie leur propre securite en tant que passager, aux depens de celle des pietons. Cette preference peut amener des comportements peu rationnels. Ainsi, en tant qu'acheteur potentiel d'une voiture autonome, nous pouvons nous montrer trop sensibles aux risques relatifs (c'est-a-dire la facon dont les risques sont distribues entre nous et les autres en cas d'accident). Et, a l'inverse, nous oublions la notion de risque absolu (c'est-a-dire la probabilite qu'un accident survienne en premier lieu). Or, ce risque absolu pourrait etre jusqu'a dix fois inferieur pour une voiture autonome que pour un conducteur humain (3). Cette difficulte a integrer les deux types de risque, et surtout leur importance respective, est problematique. Des consommateurs pourraient etre rebutes par un risque relatif plus eleve (c'est-a-dire le fait que la voiture ne les protege pas toujours en cas de dilemme) sans realiser que la voiture autonome diminue enormement la probabilite qu'une telle situation se produise ! A cette difficulte, s'ajoute celle de maintenir la confiance des utilisateurs dans un systeme autonome alors que ce systeme commettra immanquablement des erreurs. En effet, les gens perdent plus vite confiance si une machine dotee d'intelligence artificielle fait une erreur, meme petite, alors qu'ils pardonnent une erreur commise par un autre etre humain. Nous pardonnons leurs erreurs aux humains car nous supposons qu'ils s'efforcent de ne pas les reproduire. Mais si un algorithme fait une erreur, alors la mefiance s'installe sur la facon dont il est programme. Cette severite n'est pas toujours fondee (car les algorithmes peuvent apprendre de leurs erreurs et s'ameliorer au fil de leur utilisation), mais elle sera un parametre important a considerer avant la mise en circulation des voitures autonomes. Meme si nous parvenons a clarifier dans l'esprit du public les notions de risque relatif et absolu, et a dissiper certaines idees fausses sur les capacites des algorithmes, le debat sur l'ethique des voitures autonomes sera loin d'etre tranche. Ainsi, le ministere des Transports allemand a presente en septembre un rapport realise par des experts en intelligence artificielle, en droit et en philosophie, concluant qu'il serait moralement inacceptable pour une voiture de choisir qui elle doit sauver sur la base de caracteristiques telles que l'age ou le sexe. Interdiction, donc, de sauver un enfant au prix de la vie d'un adulte. Fort bien. Notre role n'est pas de debattre des merites ethiques de cette recommandation. Mais notre projet Moral Machine permet de comprendre ce a quoi les gens s'attendent et ce qui risque de les scandaliser. Et au vu de nos resultats, il est probable que l'opinion soit scandalisee le jour ou un enfant sera sacrifie pour ne pas risquer la vie d'un adulte. Il faut comprendre que meme si des decisions ont ete prises sur des criteres ethiques respectables, elles conduiront a certains accidents qui genereront un outrage public et qui compromettront l'acceptabilite et l'adoption des voitures autonomes. Il s'agit donc pour nous d'anticiper ces mouvements d'opinions et d'en mesurer les risques, et non de peser sur les legislateurs afin qu'ils collent a la volonte des gens telle qu'elle est mesuree avec notre simulateur. (1) Edmond Awad, Jean-Francois Bonnefon, Sohan Dsouza, Joe Henrich, Richard Kim, Iyad Rahwan, Azim Shariff, Jonathan Schulz. Le projet Moral Machine est ne de notre premiere publication sur le sujet : <<The Social Dilemma of Autonomous Vehicles>>,  Science,  de Bonnefon, Shariff et Rahwan (2016).  (2) Selon les projections les plus optimistes, la voiture autonome deployee a large echelle permettrait d'eviter 90 % des accidents car 90 % des accidents actuels sont lies a une erreur humaine (conducteur fatigue, en etat d'ebriete, qui ne respecte pas le code de la route, etc.). <<Algorithm Aversion : People Erroneously Avoid Algorithms After Seeing Them Err.>>  Journal of Experimental Psychology : General,  de Berkeley J. Dietvorst, Joseph P. Simmons, et Cade Massey (2014).  (3) <<Psychological Roadblocks to the Adoption of Self-Driving Vehicles>>,  Nature Human Behaviour,  de Shariff, Bonnefon et Rahwan (2017).", "theme": "debats"}